{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFilter, ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_w, img_h = 58, 40\n",
    "bg_w, bg_h = 60, 60\n",
    "channel = 3\n",
    "\n",
    "offsets = [((bg_w-img_w)/2, (bg_h-img_h)/2)]\n",
    "\n",
    "# http://blog.csdn.net/icamera0/article/details/50708888\n",
    "filters = [None, ImageFilter.GaussianBlur, ImageFilter.BLUR, ImageFilter.EDGE_ENHANCE_MORE, ImageFilter.DETAIL, ImageFilter.SHARPEN]\n",
    "\n",
    "angles = range(0, 360, 45)\n",
    "\n",
    "X_images = []\n",
    "Y_images = []\n",
    "\n",
    "def scan_files():\n",
    "    for filepath in glob.glob(\"/Users/chia/Downloads/HPEImages/*\"):\n",
    "        filename = os.path.basename(filepath)\n",
    "        y = filepath[filepath.find(\"/\")+1:filepath.find(\".\")-1]\n",
    "        \n",
    "        img = Image.open(filepath)        \n",
    "        ori_size = img.size\n",
    "        is_horizontal = True if ori_size[0] > ori_size[1] else False\n",
    "        \n",
    "        if is_horizontal:\n",
    "            img = img.resize((img_w, img_h), Image.ANTIALIAS)\n",
    "        else:\n",
    "            img = img.resize((img_h, img_w), Image.ANTIALIAS)\n",
    "            \n",
    "        for filter in filters:\n",
    "            for offset in offsets:\n",
    "                new_img = Image.new(\"RGB\", (bg_w, bg_h), (255, 255, 255))\n",
    "                new_img.paste(img.filter(filter) if filter else img, (offset[0], offset[1]) if is_horizontal else (offset[1], offset[0]))\n",
    "                for angle in angles:\n",
    "                    X_images.append(new_img.rotate(angle))\n",
    "                    Y_images.append(y)\n",
    "\n",
    "    print (len(X_images), len(Y_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "integer argument expected, got float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-13755d672088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscan_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-8aaf6d20657f>\u001b[0m in \u001b[0;36mscan_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mnew_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbg_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mnew_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_horizontal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mangle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mangles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mX_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chia/anaconda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mpaste\u001b[0;34m(self, im, box, mask)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaste\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: integer argument expected, got float"
     ]
    }
   ],
   "source": [
    "scan_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = {}\n",
    "\n",
    "def one_hot(y):\n",
    "    for sub_y in y:\n",
    "        if sub_y not in names:\n",
    "            names[sub_y] = len(names)\n",
    "            \n",
    "    ret = np.zeros(shape=(len(y), len(names)), dtype=np.int32)\n",
    "    for idx, sub_y in enumerate(y):\n",
    "        ret[idx, names[sub_y]] = 1\n",
    "    \n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 60, 60, 3)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for x, y in zip(X_images, Y_images):\n",
    "    weight, height = x.size\n",
    "    pixels = x.load()\n",
    "    \n",
    "    for w in range(weight):\n",
    "        for h in range(height):\n",
    "            X.append(pixels[w, h])\n",
    "            \n",
    "X = (np.array(X, dtype=np.float32)/255).reshape((-1, bg_w, bg_h, channel))\n",
    "print (X.shape)\n",
    "\n",
    "print(X_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "Y = one_hot(Y_images)\n",
    "print (Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset to Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 60, 60, 3) (0, 60, 60, 3)\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "#rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)\n",
    "\n",
    "#train_idx, test_idx = None, None\n",
    "#for training_idx, testing_idx in rs.split(X):\n",
    "#    pass\n",
    "\n",
    "# The real images are 'TESTING' dataset; the fake(generated by programes) are 'TRAINING' dataset\n",
    "\n",
    "training_idx = [idx for idx in range(len(X)) if idx%(len(offsets)*len(angles)*len(filters))!=0]\n",
    "testing_idx = [idx for idx in range(len(X)) if idx%(len(offsets)*len(angles)*len(filters))==0]\n",
    "\n",
    "dataset_training_x, dataset_training_y = X[training_idx], Y[training_idx]\n",
    "dataset_testing_x, dataset_testing_y = X[testing_idx], Y[testing_idx]\n",
    "\n",
    "print (dataset_training_x.shape, dataset_testing_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "def get_weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1))\n",
    "\n",
    "def get_bias(shape, value=0.1):\n",
    "    return tf.Variable(tf.constant(value, shape=shape))\n",
    "\n",
    "def get_conv2d(x, shape, value=0.1, padding=\"SAME\"):\n",
    "    with tf.name_scope(\"conv_weight\") as scope:\n",
    "        w = get_weight(shape)\n",
    "        variable_summaries(w)\n",
    "        \n",
    "    with tf.name_scope(\"conv_bias\") as scope:\n",
    "        b = get_bias([shape[3]]) \n",
    "        variable_summaries(b)\n",
    "    \n",
    "    return tf.nn.relu(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=padding) + b)\n",
    "\n",
    "def get_max_pooling2x2(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"):\n",
    "    return tf.nn.max_pool(conv, \n",
    "                          ksize=ksize, \n",
    "                          strides=strides, \n",
    "                          padding=padding)\n",
    "\n",
    "def get_full_layer(data, shape):\n",
    "    w = get_weight(shape)\n",
    "    b = get_bias([shape[-1]])\n",
    "    \n",
    "    return tf.matmul(data, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the images based on the batch-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(dataset_x, dataset_y, batch_size=8):\n",
    "    from random import shuffle\n",
    "    \n",
    "    ret = range(0, len(dataset_y))\n",
    "    shuffle(ret)\n",
    "    ret = np.array(ret)\n",
    "    \n",
    "    for idx in range((len(dataset_y)/batch_size)+1):\n",
    "        start_idx = idx*batch_size\n",
    "        end_idx = min((idx+1)*batch_size, len(dataset_y))\n",
    "        \n",
    "        yield dataset_x[ret[start_idx:end_idx]], dataset_y[ret[start_idx:end_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim_input = bg_w\n",
    "dim_dense = 1024\n",
    "dim_output = len(names)\n",
    "\n",
    "batch_size=32\n",
    "nepoch = 256\n",
    "printing_epoch = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-8a5a03a75e4a>, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-8a5a03a75e4a>\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    print \"\\tTraining/Testing Accuracy: {}%/{}%, {} seconds\".format( round(acc_training,4), round(acc_testing,4), round(time.time()-timestamp_start, 4))\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "_x = tf.placeholder(tf.float32, shape=[None, dim_input, dim_input, channel])\n",
    "_y = tf.placeholder(tf.float32, shape=[None, dim_output])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "conv_pixel = 5\n",
    "conv_padding = \"SAME\"\n",
    "\n",
    "conv1_feature_map = 32\n",
    "conv1 = get_conv2d(_x, [conv_pixel, conv_pixel, channel, conv1_feature_map])\n",
    "conv1_pooling = get_max_pooling2x2(conv1)\n",
    "conv1_drop = tf.nn.dropout(conv1_pooling, keep_prob=keep_prob)\n",
    "\n",
    "conv2_feature_map = 64\n",
    "conv2 = get_conv2d(conv1_drop, [conv_pixel, conv_pixel, conv1_feature_map, conv2_feature_map])\n",
    "conv2_pooling = get_max_pooling2x2(conv2)\n",
    "conv2_flat = tf.reshape(conv2_pooling, [-1, bg_w/2/2*bg_w/2/2*conv2_feature_map])\n",
    "conv2_drop = tf.nn.dropout(conv2_flat, keep_prob=keep_prob)\n",
    "\n",
    "full_layer = tf.nn.relu(get_full_layer(conv2_drop, [bg_w/2/2*bg_w/2/2*conv2_feature_map, dim_dense]))\n",
    "full_drop = tf.nn.dropout(full_layer, keep_prob=keep_prob)\n",
    "\n",
    "final_layer = get_full_layer(full_drop, [dim_dense, dim_output])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=final_layer, labels=_y)\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(final_layer, 1), tf.argmax(_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    timestamp_start = time.time()\n",
    "    for idx in range(nepoch):\n",
    "        for train_x, train_y in next_batch(dataset_training_x, dataset_training_y, batch_size=batch_size):\n",
    "            sess.run([train_step], feed_dict={_x: train_x, _y: train_y, keep_prob: 0.5})\n",
    "\n",
    "        if idx%printing_epoch == 0 or idx==nepoch-1:\n",
    "            print (\"epoch at {}\".format(idx+1))\n",
    "            acc_training = accuracy.eval(feed_dict={_x: dataset_training_x, \n",
    "                                                    _y: dataset_training_y, \n",
    "                                                    keep_prob:1.0})\n",
    "            acc_testing = accuracy.eval(feed_dict={_x: dataset_testing_x, \n",
    "                                                   _y: dataset_testing_y, \n",
    "                                                   keep_prob:1.0})\n",
    "            print \"\\tTraining/Testing Accuracy: {}%/{}%, {} seconds\".format( round(acc_training,4), round(acc_testing,4), round(time.time()-timestamp_start, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LRN after Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch at 1\n",
      "\tTraining/Testing Accuracy: 16.2948%/17.0732%, 32.7216 seconds\n",
      "epoch at 17\n",
      "\tTraining/Testing Accuracy: 39.8547%/31.7073%, 430.8147 seconds\n",
      "epoch at 33\n",
      "\tTraining/Testing Accuracy: 59.5745%/63.4146%, 826.3669 seconds\n",
      "epoch at 49\n",
      "\tTraining/Testing Accuracy: 70.109%/80.4878%, 1219.3636 seconds\n",
      "epoch at 65\n",
      "\tTraining/Testing Accuracy: 90.3477%/95.1219%, 1612.0352 seconds\n",
      "epoch at 81\n",
      "\tTraining/Testing Accuracy: 94.9144%/97.561%, 2005.1507 seconds\n",
      "epoch at 97\n",
      "\tTraining/Testing Accuracy: 98.0799%/100.0%, 2396.9534 seconds\n",
      "epoch at 113\n",
      "\tTraining/Testing Accuracy: 99.4292%/100.0%, 2785.9088 seconds\n",
      "epoch at 129\n",
      "\tTraining/Testing Accuracy: 99.9481%/100.0%, 3173.7932 seconds\n",
      "epoch at 145\n",
      "\tTraining/Testing Accuracy: 99.8962%/100.0%, 3560.9024 seconds\n",
      "epoch at 161\n",
      "\tTraining/Testing Accuracy: 99.8443%/100.0%, 3946.9674 seconds\n",
      "epoch at 177\n",
      "\tTraining/Testing Accuracy: 99.9481%/100.0%, 4332.7239 seconds\n",
      "epoch at 193\n",
      "\tTraining/Testing Accuracy: 99.9481%/100.0%, 4717.4697 seconds\n",
      "epoch at 209\n",
      "\tTraining/Testing Accuracy: 100.0%/100.0%, 5101.7636 seconds\n",
      "epoch at 225\n",
      "\tTraining/Testing Accuracy: 100.0%/100.0%, 5485.0176 seconds\n",
      "epoch at 241\n",
      "\tTraining/Testing Accuracy: 99.7924%/100.0%, 5877.4526 seconds\n",
      "epoch at 256\n",
      "\tTraining/Testing Accuracy: 100.0%/100.0%, 6239.5164 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "_x = tf.placeholder(tf.float32, shape=[None, dim_input, dim_input, channel])\n",
    "_y = tf.placeholder(tf.float32, shape=[None, dim_output])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "conv_pixel = 5\n",
    "conv_padding = \"SAME\"\n",
    "\n",
    "conv1_feature_map = 32\n",
    "conv1 = get_conv2d(_x, [conv_pixel, conv_pixel, channel, conv1_feature_map])\n",
    "conv1_pooling = get_max_pooling2x2(conv1)\n",
    "conv1_norm = tf.nn.lrn(conv1_pooling, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75)\n",
    "conv1_drop = tf.nn.dropout(conv1_norm, keep_prob=keep_prob)\n",
    "\n",
    "conv2_feature_map = 64\n",
    "conv2 = get_conv2d(conv1_drop, [conv_pixel, conv_pixel, conv1_feature_map, conv2_feature_map])\n",
    "conv2_pooling = get_max_pooling2x2(conv2)\n",
    "conv2_norm = tf.nn.lrn(conv2_pooling, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75)\n",
    "conv2_flat = tf.reshape(conv2_norm, [-1, bg_w/2/2*bg_w/2/2*conv2_feature_map])\n",
    "conv2_drop = tf.nn.dropout(conv2_flat, keep_prob=keep_prob)\n",
    "\n",
    "full_layer = tf.nn.relu(get_full_layer(conv2_drop, [bg_w/2/2*bg_w/2/2*conv2_feature_map, dim_dense]))\n",
    "full_drop = tf.nn.dropout(full_layer, keep_prob=keep_prob)\n",
    "\n",
    "final_layer = get_full_layer(full_drop, [dim_dense, dim_output])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=final_layer, labels=_y)\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(final_layer, 1), tf.argmax(_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    timestamp_start = time.time()\n",
    "    for idx in range(nepoch):\n",
    "        for train_x, train_y in next_batch(dataset_training_x, dataset_training_y, batch_size=batch_size):\n",
    "            sess.run([train_step], feed_dict={_x: train_x, _y: train_y, keep_prob: 0.5})\n",
    "\n",
    "        if idx%printing_epoch == 0 or idx==nepoch-1:\n",
    "            print \"epoch at {}\".format(idx+1)\n",
    "            acc_training = accuracy.eval(feed_dict={_x: dataset_training_x, \n",
    "                                                    _y: dataset_training_y, \n",
    "                                                    keep_prob:1.0})\n",
    "            acc_testing = accuracy.eval(feed_dict={_x: dataset_testing_x, \n",
    "                                                   _y: dataset_testing_y, \n",
    "                                                   keep_prob:1.0})\n",
    "            print \"\\tTraining/Testing Accuracy: {}%/{}%, {} seconds\".format(\\\n",
    "                round(acc_training,4), round(acc_testing,4), round(time.time()-timestamp_start, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch at 1\n",
      "\tTraining/Testing Accuracy: 24.2346%/36.5854%, 43.9442 seconds\n",
      "epoch at 17\n",
      "\tTraining/Testing Accuracy: 22.1588%/24.3902%, 574.3647 seconds\n",
      "epoch at 33\n",
      "\tTraining/Testing Accuracy: 16.5542%/17.0732%, 1102.7844 seconds\n",
      "epoch at 49\n",
      "\tTraining/Testing Accuracy: 15.3607%/14.6341%, 1630.9512 seconds\n",
      "epoch at 65\n",
      "\tTraining/Testing Accuracy: 14.6341%/14.6341%, 2157.9161 seconds\n",
      "epoch at 81\n",
      "\tTraining/Testing Accuracy: 14.6341%/14.6341%, 2684.1291 seconds\n",
      "epoch at 97\n",
      "\tTraining/Testing Accuracy: 14.6341%/14.6341%, 3210.3743 seconds\n",
      "epoch at 113\n",
      "\tTraining/Testing Accuracy: 14.6341%/14.6341%, 3736.9272 seconds\n",
      "epoch at 129\n",
      "\tTraining/Testing Accuracy: 14.6341%/14.6341%, 4263.2824 seconds\n",
      "epoch at 145\n",
      "\tTraining/Testing Accuracy: 14.6341%/14.6341%, 4790.0198 seconds\n",
      "epoch at 161\n",
      "\tTraining/Testing Accuracy: 14.6341%/14.6341%, 5316.8031 seconds\n",
      "epoch at 177\n",
      "\tTraining/Testing Accuracy: 15.6201%/17.0732%, 5843.0808 seconds\n",
      "epoch at 193\n",
      "\tTraining/Testing Accuracy: 15.205%/17.0732%, 6369.7467 seconds\n",
      "epoch at 209\n",
      "\tTraining/Testing Accuracy: 16.3985%/21.9512%, 6896.1589 seconds\n",
      "epoch at 225\n",
      "\tTraining/Testing Accuracy: 11.3129%/12.1951%, 7422.4573 seconds\n",
      "epoch at 241\n",
      "\tTraining/Testing Accuracy: 13.5963%/14.6341%, 7955.1638 seconds\n",
      "epoch at 256\n",
      "\tTraining/Testing Accuracy: 16.3985%/21.9512%, 8453.451 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "_x = tf.placeholder(tf.float32, shape=[None, dim_input, dim_input, channel])\n",
    "_y = tf.placeholder(tf.float32, shape=[None, dim_output])\n",
    "is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "conv_pixel = 5\n",
    "conv_padding = \"SAME\"\n",
    "\n",
    "conv1_feature_map = 32\n",
    "conv1 = get_conv2d(_x, [conv_pixel, conv_pixel, channel, conv1_feature_map])\n",
    "\n",
    "#http://ruishu.io/2016/12/27/batchnorm/\n",
    "conv1_norm = tf.contrib.layers.batch_norm(conv1, center=True, scale=True, is_training=is_training)\n",
    "conv1_pooling = get_max_pooling2x2(conv1_norm)\n",
    "conv1_drop = tf.nn.dropout(conv1_pooling, keep_prob=keep_prob)\n",
    "\n",
    "conv2_feature_map = 64\n",
    "conv2 = get_conv2d(conv1_drop, [conv_pixel, conv_pixel, conv1_feature_map, conv2_feature_map])\n",
    "\n",
    "#http://ruishu.io/2016/12/27/batchnorm/\n",
    "conv2_norm = tf.contrib.layers.batch_norm(conv2, center=True, scale=True, is_training=is_training)\n",
    "conv2_pooling = get_max_pooling2x2(conv2_norm)\n",
    "conv2_flat = tf.reshape(conv2_pooling, [-1, bg_w/2/2*bg_w/2/2*conv2_feature_map])\n",
    "conv2_drop = tf.nn.dropout(conv2_flat, keep_prob=keep_prob)\n",
    "\n",
    "full_layer = tf.nn.relu(get_full_layer(conv2_drop, [bg_w/2/2*bg_w/2/2*conv2_feature_map, dim_dense]))\n",
    "full_drop = tf.nn.dropout(full_layer, keep_prob=keep_prob)\n",
    "\n",
    "final_layer = get_full_layer(full_drop, [dim_dense, dim_output])\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=final_layer, labels=_y)\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(final_layer, 1), tf.argmax(_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    timestamp_start = time.time()\n",
    "    for idx in range(nepoch):\n",
    "        for train_x, train_y in next_batch(dataset_training_x, dataset_training_y, batch_size=batch_size):\n",
    "            sess.run([train_step], feed_dict={_x: train_x, _y: train_y, keep_prob: 0.5, is_training: True})\n",
    "\n",
    "        if idx%printing_epoch == 0 or idx==nepoch-1:\n",
    "            print \"epoch at {}\".format(idx+1)\n",
    "            acc_training = accuracy.eval(feed_dict={_x: dataset_training_x, \n",
    "                                                    _y: dataset_training_y, \n",
    "                                                    keep_prob:1.0, \n",
    "                                                    is_training: False})\n",
    "            acc_testing = accuracy.eval(feed_dict={_x: dataset_testing_x, \n",
    "                                                   _y: dataset_testing_y, \n",
    "                                                   keep_prob:1.0, \n",
    "                                                   is_training: False})\n",
    "            print \"\\tTraining/Testing Accuracy: {}%/{}%, {} seconds\".format(\\\n",
    "                round(acc_training,4), round(acc_testing,4), round(time.time()-timestamp_start, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch at 1\n",
      "\tTraining/Testing Accuracy: 12.1951%/12.1951%, 73.3212 seconds\n",
      "epoch at 17\n",
      "\tTraining/Testing Accuracy: 20.9652%/19.5122%, 815.3655 seconds\n",
      "epoch at 33\n",
      "\tTraining/Testing Accuracy: 60.9756%/63.4146%, 1549.7709 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "_x = tf.placeholder(tf.float32, shape=[None, dim_input, dim_input, channel])\n",
    "_y = tf.placeholder(tf.float32, shape=[None, dim_output])\n",
    "is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "conv_pixel = 5\n",
    "conv_padding = \"SAME\"\n",
    "\n",
    "conv1_feature_map = 32\n",
    "conv1 = get_conv2d(_x, [conv_pixel, conv_pixel, channel, conv1_feature_map])\n",
    "\n",
    "#http://ruishu.io/2016/12/27/batchnorm/\n",
    "\n",
    "conv1_pooling = get_max_pooling2x2(conv1)\n",
    "conv1_norm = tf.contrib.layers.batch_norm(conv1_pooling, center=True, scale=True, is_training=is_training)\n",
    "conv1_drop = tf.nn.dropout(conv1_norm, keep_prob=keep_prob)\n",
    "\n",
    "conv2_feature_map = 64\n",
    "conv2 = get_conv2d(conv1_drop, [conv_pixel, conv_pixel, conv1_feature_map, conv2_feature_map])\n",
    "\n",
    "#http://ruishu.io/2016/12/27/batchnorm/\n",
    "\n",
    "conv2_pooling = get_max_pooling2x2(conv2)\n",
    "conv2_norm = tf.contrib.layers.batch_norm(conv2_pooling, center=True, scale=True, is_training=is_training)\n",
    "conv2_flat = tf.reshape(conv2_norm, [-1, bg_w/2/2*bg_w/2/2*conv2_feature_map])\n",
    "conv2_drop = tf.nn.dropout(conv2_flat, keep_prob=keep_prob)\n",
    "\n",
    "full_layer = tf.nn.relu(get_full_layer(conv2_drop, [bg_w/2/2*bg_w/2/2*conv2_feature_map, dim_dense]))\n",
    "full_drop = tf.nn.dropout(full_layer, keep_prob=keep_prob)\n",
    "\n",
    "final_layer = get_full_layer(full_drop, [dim_dense, dim_output])\n",
    "\n",
    "bn_final_layer = tf.contrib.layers.batch_norm(final_layer, center=True, scale=True, is_training=is_training)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=bn_final_layer, labels=_y)\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(final_layer, 1), tf.argmax(_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    timestamp_start = time.time()\n",
    "    for idx in range(nepoch):\n",
    "        for train_x, train_y in next_batch(dataset_training_x, dataset_training_y, batch_size=batch_size):\n",
    "            sess.run([train_step], feed_dict={_x: train_x, _y: train_y, keep_prob: 0.5, is_training: True})\n",
    "\n",
    "        if idx%printing_epoch == 0 or idx==nepoch-1:\n",
    "            print \"epoch at {}\".format(idx+1)\n",
    "            acc_training = accuracy.eval(feed_dict={_x: dataset_training_x, \n",
    "                                                    _y: dataset_training_y, \n",
    "                                                    keep_prob:1.0, \n",
    "                                                    is_training: False})\n",
    "            acc_testing = accuracy.eval(feed_dict={_x: dataset_testing_x, \n",
    "                                                   _y: dataset_testing_y, \n",
    "                                                   keep_prob:1.0, \n",
    "                                                   is_training: False})\n",
    "            print \"\\tTraining/Testing Accuracy: {}%/{}%, {} seconds\".format(\\\n",
    "                round(acc_training,4), round(acc_testing,4), round(time.time()-timestamp_start, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
